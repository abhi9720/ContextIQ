
# Retrieval-Augmented Generation (RAG) System

This document provides a technical overview of the RAG system, a FastAPI-based application that leverages large language models (LLMs) to answer questions based on a corpus of documents.

## Overview

The system is designed to be a modular and extensible platform for building and deploying RAG applications. It consists of two main pipelines: an ingestion pipeline for processing and embedding documents, and a retrieval pipeline for answering user queries.

## System Architecture

The application is divided into three main layers: the API layer, the pipeline layer, and the storage layer.

```mermaid
--- 
config: 
  theme: base
  layout: fixed
--- 
flowchart TD

%% ======================= CLIENT LAYER =======================
subgraph CLIENT["ðŸ§‘â€ðŸ’» Client Applications"]
  WebApp["Web / Mobile / Admin UI"]
  UploadModule["Document Upload / Sync Module"]
  ChatModule["Chat + Search Interface"]
  Dashboard["Admin Dashboard (Monitor & Manage Docs)"]
end

%% ======================= BACKEND LAYER =======================
subgraph BACKEND["ðŸŒ Backend API (FastAPI / Flask)"]

  %% -------- DOCUMENT INGESTION PIPELINE --------
  subgraph INGESTION["ðŸ“¥ Document Ingestion Pipeline"]
    FileUploadAPI["POST /embed â†’ Upload Document"]
    FileValidator["Validator â†’ Type, Size, Integrity"]
    SourceRouter["Source Router â†’ File / URL / API / Stream"]
    TextExtractor["Text Extractor â†’ PDF, DOCX, HTML, Email"]
    TextPreprocessor["Text Preprocessor â†’ Cleanup, Normalize, Lemmatize"]
    Chunker["Chunker â†’ Token / Paragraph Split + Overlap"]
    MetadataGenerator["Metadata Generator â†’ Title, Author, Tags"]
    EmbeddingClient["Embedding Model Client â†’ SentenceTransformer / OpenAI"]
    ChromaWriter["Chroma Client â†’ Upsert Vectors + Metadata"]
    MetadataWriter["Metadata Writer â†’ MongoDB / Postgres"]
    FileStorageWriter["Blob Storage Writer â†’ S3 / NFS / Local Disk"]
  end

  %% -------- RETRIEVAL PIPELINE --------
  subgraph RETRIEVAL["ðŸ” Retrieval & Query Pipeline"]
    QueryAPI["POST /retrieve â†’ Semantic Search"]
    QueryValidator["Query Validator â†’ Length, Language, Safety"]
    QueryEmbedder["Query Embedder â†’ Same Model as Document Embeddings"]
    VectorRetriever["Vector Search â†’ Chroma (Cosine Similarity)"]
    ResultRanker["Re-ranker â†’ Cross-Encoder / BGE / Score Normalizer"]
    ContextAssembler["Context Assembler â†’ Merge + Deduplicate"]
    ContextCompressor["Context Compressor â†’ Token Optimization"]
    MetadataFetcher["Metadata Fetcher â†’ Title, Source, Timestamp"]
  end

  %% -------- LLM ORCHESTRATION --------
  subgraph LLM_FLOW["ðŸ§  LLM Orchestration & Response Generation"]
    PromptTemplateManager["Prompt Template Manager â†’ Q&A / Summary / Search"]
    ContextEnhancer["Context Enhancer â†’ Add Metadata + Highlights"]
    SafetyFilter["Safety Filter â†’ Sensitive Data Masking"]
    PromptComposer["Prompt Composer â†’ Build Final System + User Prompt"]
    LLMInvoker["LLM Connector â†’ Gemini / GPT / Claude"]
    ResponseParser["Response Parser â†’ Structured / Plain Text"]
    ResponseEnhancer["Response Enhancer â†’ Formatting + Source Linking"]
    FeedbackHandler["Feedback Collector â†’ User Ratings + Corrections"]
  end
end

%% ======================= STORAGE LAYER =======================
subgraph STORAGE["ðŸ—‚ï¸ Storage & Database Layer"]
  ChromaDB["ChromaDB â†’ Vector Store"]
  MongoDB["MongoDB / Postgres â†’ Metadata + User Data"]
  BlobStorage["S3 / NFS / Local Disk â†’ Raw Documents"]
end

%% ======================= MODEL LAYER =======================
subgraph MODEL["ðŸ¤– Models"]
  EmbedModel["Embedding Model â†’ text-embedding-3-small / all-MiniLM-L6-v2"]
  LLM["LLM â†’ Gemini / GPT-4 / Claude"]
end


%% ======================= DATA FLOWS =======================

%% ---- Ingestion Flow ----
WebApp -->|Uploads / Syncs Documents| UploadModule
UploadModule --> FileUploadAPI
FileUploadAPI --> FileValidator
FileValidator --> SourceRouter
SourceRouter --> TextExtractor
TextExtractor --> TextPreprocessor
TextPreprocessor --> Chunker
Chunker --> MetadataGenerator
MetadataGenerator --> EmbeddingClient
EmbeddingClient -->|Generate Vector Embeddings| ChromaWriter
ChromaWriter -->|Store Embeddings| ChromaDB
MetadataWriter --> MongoDB
TextExtractor -->|Save Raw Files| FileStorageWriter
FileStorageWriter --> BlobStorage

%% ---- Retrieval Flow ----
ChatModule -->|User Query| QueryAPI
QueryAPI --> QueryValidator
QueryValidator --> QueryEmbedder
QueryEmbedder -->|Generate Query Embedding| VectorRetriever
VectorRetriever -->|Retrieve Top-K Matches| ResultRanker
ResultRanker --> ContextAssembler
ContextAssembler --> ContextCompressor
ContextCompressor --> MetadataFetcher
MetadataFetcher --> PromptTemplateManager
PromptTemplateManager --> ContextEnhancer
ContextEnhancer --> SafetyFilter
SafetyFilter --> PromptComposer
PromptComposer --> LLMInvoker
LLMInvoker -->|Invoke LLM| LLM
LLM -->|Generate Response| ResponseParser
ResponseParser --> ResponseEnhancer
ResponseEnhancer -->|Return Final Answer| ChatModule
FeedbackHandler -->|User Ratings / Feedback| MongoDB
MongoDB -->|Improve Ranking / Quality| ChromaDB
```

### Ingestion Pipeline

The ingestion pipeline is responsible for processing and embedding documents. It consists of the following steps:

1.  **File Validation:** Validates the uploaded file type, size, and integrity.
2.  **Text Extraction:** Extracts text from the document using the appropriate loader (e.g., `PyPDFLoader` for PDFs, `Docx2txtLoader` for DOCX files).
3.  **Text Chunking:** Splits the extracted text into smaller, overlapping chunks using the `RecursiveCharacterTextSplitter`.
4.  **Metadata Generation:** Generates metadata for each chunk, including the document ID, paragraph ID, and other relevant information.
5.  **Embedding and Storage:** Generates vector embeddings for each chunk and stores them in a ChromaDB vector store.

### Retrieval Pipeline

The retrieval pipeline is responsible for answering user queries. It consists of the following steps:

1.  **Query Validation:** Validates the user's query for length, language, and safety.
2.  **Retrieval:** Retrieves the top-k most relevant text chunks from the vector store based on the user's query.
3.  **Reranking:** Reranks the retrieved chunks using a cross-encoder to improve relevance.
4.  **Context Assembly:** Assembles the context from the reranked chunks.
5.  **Context Compression:** Compresses the context to optimize for the LLM's context window.
6.  **Prompt Composition:** Composes a prompt using the compressed context and the user's query.
7.  **LLM Invocation:** Invokes the LLM to generate a response.
8.  **Response Enhancement:** Enhances the response by adding sources and other relevant information.

## API Reference

### `POST /embed`

Uploads a document for processing and embedding.

**Request:**

```bash
curl -X POST -F "file=@/path/to/your/file.pdf" http://localhost:3000/embed
```

**Response:**

```json
{
  "filename": "file.pdf",
  "doc_id": "...",
  "num_chunks": 0
}
```

### `POST /retrieve`

Answers a user's query based on the documents in the vector store.

**Request:**

```bash
curl -X POST -H "Content-Type: application/json" -d '{"query": "your query"}' http://localhost:3000/retrieve
```

**Response:**

```json
{
  "answer": "...",
  "sources": [
    {
      "source": "...",
      "doc_id": "..."
    }
  ]
}
```

## Local Development

To set up the project for local development, follow these steps:

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/GoogleCloudPlatform/genai-for-marketing
    cd genai-for-marketing
    ```

2.  **Create and activate a virtual environment:**

    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Install the dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up your environment variables:**

    Create a `.env` file in the root of the project and add your Gemini API key:

    ```
    GEMINI_API_KEY="YOUR_API_KEY"
    ```

5.  **Run the application:**

    ```bash
    uvicorn src.main:app --host 0.0.0.0 --port 3000
    ```
